{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "zF04K1isgZT7",
        "ty4M6MOEYglJ"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports & Installation"
      ],
      "metadata": {
        "id": "QAUkFuOFd6Em"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Kz7Iz6qmifI",
        "outputId": "9e9bd5fe-ab06-441c-e14e-4a67fc92f568"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "NDlTI0dSZPj8",
        "outputId": "bd39196f-b3f1-4976-e0e0-5cd47a9c2018"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.5.1-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.5.1-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.4/491.4 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.1 dill-0.3.8 fsspec-2025.3.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5JakdcGBdgM9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "import json\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, EarlyStoppingCallback\n",
        "from transformers import Trainer, TrainingArguments\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datasets import load_from_disk, load_dataset, Dataset\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (\n",
        "    f1_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    accuracy_score,\n",
        "    precision_recall_fscore_support,\n",
        "    classification_report\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "tNEQZfhK9qdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read in Data\n",
        "Assumes that the data is saved as three different Arrow-type datasets."
      ],
      "metadata": {
        "id": "zF04K1isgZT7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load datasets (non-tokenized)\n",
        "# 'text' is list of strings\n",
        "# 'labels' is list of integers, positive = 1 = arxiv\n",
        "# train, val, & test all have 50/50 distribution of arxiv and vixra examples\n",
        "DATA_DIR = '/content/drive/MyDrive/sp25/paper-moderation-SP25/'\n",
        "train_dataset = load_from_disk(DATA_DIR + \"/physics_subset_train.arrow\")\n",
        "val_dataset = load_from_disk(DATA_DIR + \"/physics_subset_val.arrow\")\n",
        "test_dataset = load_from_disk(DATA_DIR + \"/physics_subset_test.arrow\")"
      ],
      "metadata": {
        "id": "ufuDDZ9FQFaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bag of Words Baseline"
      ],
      "metadata": {
        "id": "wazLyIsv4zsZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = 10000\n",
        "val_size = 2000\n",
        "\n",
        "X_train = train_dataset['text'][:train_size]\n",
        "X_test = val_dataset['text'][:val_size]\n",
        "y_train = train_dataset['labels'][:train_size]\n",
        "y_test = val_dataset['labels'][:val_size]"
      ],
      "metadata": {
        "id": "bSDnuAZ55Me2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_clf = Pipeline([\n",
        "    ('vect', CountVectorizer(stop_words=list(stopwords.words('english')), ngram_range=(1, 2), min_df=0.05)),\n",
        "    ('clf', LogisticRegression(random_state=42, max_iter=1000))\n",
        "])\n",
        "# Train the model\n",
        "text_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = text_clf.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62XnWtAf41Ny",
        "outputId": "5e8894b2-c8a8-4944-9e1a-e9460353400e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.956\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1065   37]\n",
            " [  51  847]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.97      0.96      1102\n",
            "           1       0.96      0.94      0.95       898\n",
            "\n",
            "    accuracy                           0.96      2000\n",
            "   macro avg       0.96      0.95      0.96      2000\n",
            "weighted avg       0.96      0.96      0.96      2000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if hasattr(text_clf['clf'], 'coef_'):\n",
        "    feature_names = text_clf['vect'].get_feature_names_out()\n",
        "    coefs = text_clf['clf'].coef_[0]\n",
        "\n",
        "    # Create a DataFrame to sort features by coefficient value\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'feature': feature_names,\n",
        "        'coefficient': coefs\n",
        "    })\n",
        "\n",
        "    # Sort by coefficient value to get most positive and most negative features\n",
        "    feature_importance = feature_importance.sort_values('coefficient', ascending=False)\n",
        "\n",
        "    print(\"Top positive features (most predictive of positive class):\")\n",
        "    print(feature_importance.head(10).to_string(index=False))\n",
        "\n",
        "    print(\"\\nTop negative features (most predictive of negative class):\")\n",
        "    print(feature_importance.tail(10).sort_values('coefficient').to_string(index=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OM_vrYJTGmth",
        "outputId": "9d01805e-6a32-4e92-ef28-05a67881ecad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top positive features (most predictive of positive class):\n",
            "   feature  coefficient\n",
            "     video     0.779024\n",
            "       cm2     0.488253\n",
            "       sys     0.433185\n",
            "      tion     0.429000\n",
            "      view     0.423504\n",
            "       von     0.387996\n",
            "   unknown     0.380963\n",
            "references     0.374462\n",
            "    define     0.372134\n",
            "       los     0.370336\n",
            "\n",
            "Top negative features (most predictive of negative class):\n",
            " feature  coefficient\n",
            "     eld    -0.734190\n",
            "abstract    -0.683912\n",
            "    2020    -0.556857\n",
            "   chaos    -0.445842\n",
            "     org    -0.375466\n",
            "     pdf    -0.368675\n",
            "     etc    -0.363122\n",
            "   email    -0.361040\n",
            "      dr    -0.323459\n",
            "  planck    -0.306777\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RoBerTa Baseline"
      ],
      "metadata": {
        "id": "ty4M6MOEYglJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: change this path to your Google Drive Folder\n",
        "DATA_DIR = '/content/drive/MyDrive/sp25/paper-moderation-SP25'\n",
        "\n",
        "# Output directory for results\n",
        "OUTPUT_DIR = '/content/drive/MyDrive/sp25/paper-moderation-SP25/seg-baseline-results/'\n",
        "\n",
        "# RoBERTa model to use\n",
        "MODEL_NAME = \"roberta-base\"\n",
        "\n",
        "# Whether to apply text segmentation\n",
        "USE_SEGMENTATION = False\n",
        "\n",
        "# Run both with and without segmentation for comparison\n",
        "RUN_BOTH = True\n",
        "\n",
        "# Training parameters\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 2e-5\n",
        "NUM_EPOCHS = 2\n",
        "\n",
        "# Random seed for reproducibility\n",
        "SEED = 42"
      ],
      "metadata": {
        "id": "SsF7TiQuYipV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(pred):\n",
        "    \"\"\"\n",
        "    Compute metrics for model evaluation.\n",
        "\n",
        "    Args:\n",
        "        pred: Prediction output from trainer.predict()\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary of metrics\n",
        "    \"\"\"\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        labels, preds, average='macro', zero_division=0\n",
        "    )\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }"
      ],
      "metadata": {
        "id": "v-t6pXfQY211"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiment(data_dir, output_dir, model_name, use_segmentation,\n",
        "                  batch_size, learning_rate, num_epochs):\n",
        "    \"\"\"\n",
        "    Run a single experiment with or without segmentation.\n",
        "\n",
        "    Args:\n",
        "        data_dir: Directory with HuggingFace datasets\n",
        "        output_dir: Directory to save results\n",
        "        model_name: RoBERTa model name\n",
        "        use_segmentation: Whether to apply text segmentation\n",
        "        batch_size: Batch size for training\n",
        "        learning_rate: Learning rate\n",
        "        num_epochs: Number of epochs\n",
        "\n",
        "    Returns:\n",
        "        dict: Evaluation metrics\n",
        "    \"\"\"\n",
        "    # Create experiment directory\n",
        "    exp_name = f\"roberta_{model_name.split('/')[-1]}\"\n",
        "    exp_name += \"_with_segmentation\" if use_segmentation else \"_baseline\"\n",
        "    exp_dir = os.path.join(output_dir, exp_name)\n",
        "    os.makedirs(exp_dir, exist_ok=True)\n",
        "\n",
        "    print(f\"Running {'segmented' if use_segmentation else 'baseline'} experiment\")\n",
        "\n",
        "    # Load datasets (non-tokenized)\n",
        "    # 'text' is list of strings\n",
        "    # 'labels' is list of integers, positive = 1 = arxiv\n",
        "    # train, val, & test all have 50/50 distribution of arxiv and vixra examples\n",
        "    train_dataset = load_from_disk(data_dir + \"/train.arrow\")\n",
        "    val_dataset = load_from_disk(data_dir + \"/val.arrow\")\n",
        "    test_dataset = load_from_disk(data_dir + \"/test.arrow\")\n",
        "\n",
        "    print(f\"Loaded datasets - Train: {len(train_dataset)}, \"\n",
        "              f\"Validation: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
        "\n",
        "    # Load tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    # Tokenize datasets\n",
        "    def tokenize_function(examples):\n",
        "        return tokenizer(\n",
        "            examples[\"text\"],\n",
        "            padding=\"max_length\",\n",
        "            truncation=True\n",
        "        )\n",
        "\n",
        "    print(\"Tokenizing datasets.\")\n",
        "    train_ds = train_dataset.map(tokenize_function, batched=True)\n",
        "    val_ds = val_dataset.map(tokenize_function, batched=True)\n",
        "    test_ds = test_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "    train_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "    val_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "    test_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "\n",
        "    # Set up training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=os.path.join(exp_dir, \"checkpoints\"),\n",
        "        evaluation_strategy=\"epoch\",     # Keep eval after each epoch to see metrics\n",
        "        save_strategy=\"epoch\",           # Save after each epoch (needed for load_best_model_at_end)\n",
        "        save_total_limit=1,              # Only keep the most recent checkpoint\n",
        "        metric_for_best_model=\"f1\",      # Use f1 to determine best model\n",
        "        greater_is_better=True,          # Higher f1 is better\n",
        "        logging_strategy=\"steps\",        # Print logs during training\n",
        "        logging_steps=200,               # Print metrics every 200 steps\n",
        "        logging_dir=None,                # Don't save logs to disk\n",
        "        learning_rate=learning_rate,\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=batch_size,\n",
        "        num_train_epochs=num_epochs,\n",
        "        weight_decay=0.01,\n",
        "        load_best_model_at_end=True,     # Load the best model at the end of training\n",
        "        fp16=True,\n",
        "        report_to=[\"none\"],              # Don't report to any tracking systems\n",
        "    )\n",
        "\n",
        "    # Initialize model\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_name,\n",
        "        num_labels=2,\n",
        "    )\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    # Create trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_ds,\n",
        "        eval_dataset=val_ds,\n",
        "        compute_metrics=compute_metrics,\n",
        "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
        "    )\n",
        "\n",
        "    # Train model\n",
        "    print(f\"Training {'segmented' if use_segmentation else 'baseline'} model\")\n",
        "    trainer.train()\n",
        "    trainer.save_model(os.path.join(exp_dir, \"final_model\"))\n",
        "\n",
        "    # Evaluate model\n",
        "    print(f\"Evaluating {'segmented' if use_segmentation else 'baseline'} model\")\n",
        "    eval_results = trainer.evaluate(test_ds)\n",
        "\n",
        "    # Get detailed classification report\n",
        "    predictions = trainer.predict(test_ds)\n",
        "    preds = predictions.predictions.argmax(-1)\n",
        "    labels = predictions.label_ids\n",
        "    class_report = classification_report(labels, preds)\n",
        "\n",
        "    # Log and save results\n",
        "    results_str = (\n",
        "        f\"{'Segmented' if use_segmentation else 'Baseline'} Model Results:\\n\"\n",
        "        f\"Accuracy: {eval_results['eval_accuracy']:.4f}\\n\"\n",
        "        f\"Precision: {eval_results['eval_precision']:.4f}\\n\"\n",
        "        f\"Recall: {eval_results['eval_recall']:.4f}\\n\"\n",
        "        f\"F1 Score: {eval_results['eval_f1']:.4f}\\n\"\n",
        "        f\"Classification Report:\\n{class_report}\\n\"\n",
        "    )\n",
        "\n",
        "    print(results_str)\n",
        "\n",
        "    # Save results\n",
        "    with open(os.path.join(exp_dir, 'results.txt'), 'w') as f:\n",
        "        f.write(results_str)\n",
        "\n",
        "    return {\n",
        "        'accuracy': eval_results['eval_accuracy'],\n",
        "        'precision': eval_results['eval_precision'],\n",
        "        'recall': eval_results['eval_recall'],\n",
        "        'f1': eval_results['eval_f1']\n",
        "    }"
      ],
      "metadata": {
        "id": "0jY5zLVRdnj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Run experiments\n",
        "results = {}\n",
        "\n",
        "# Run baseline\n",
        "results['baseline'] = run_experiment(\n",
        "    data_dir=DATA_DIR,\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    model_name=MODEL_NAME,\n",
        "    use_segmentation=False,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    num_epochs=NUM_EPOCHS\n",
        ")\n",
        "print(\"Experiment(s) completed\")\n",
        "\n",
        "with open(os.path.join(OUTPUT_DIR, 'results.json'), 'w') as f:\n",
        "    json.dump(results, f, indent=4)\n",
        "print(results)"
      ],
      "metadata": {
        "id": "gSrUggAxd1U4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}